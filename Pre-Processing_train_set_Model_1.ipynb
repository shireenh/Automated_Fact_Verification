{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Set Pre-processing for Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Features_extraction.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import xapian\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import spacy\n",
    "import import_ipynb\n",
    "from Features_extraction import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'traing_with_text.json'  \n",
    "train_set = pd.read_json(open(train_file),orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Open the database for searching.\n",
    "database = xapian.Database(\"db-index\")\n",
    "# Start an enquire session.\n",
    "enquire = xapian.Enquire(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[]\n",
    "def get_topk_evidence(df):\n",
    "    index = df[\"index\"]\n",
    "    claim = df[\"claim\"]\n",
    "    label = df[\"label\"]\n",
    "    evidence = df[\"evidence\"]\n",
    "    query_string = claim\n",
    "    # Parse the query string to produce a Xapian::Query object.\n",
    "    qp = xapian.QueryParser()\n",
    "    stemmer = xapian.Stem(\"english\")\n",
    "    qp.set_stemmer(stemmer)\n",
    "    qp.set_database(database)\n",
    "    qp.set_stemming_strategy(xapian.QueryParser.STEM_SOME)\n",
    "    query = qp.parse_query(query_string)\n",
    "\n",
    "    # Find the top 10 results for the query.\n",
    "    enquire.set_query(query)\n",
    "    matches = enquire.get_mset(0, 5)\n",
    "\n",
    "    \n",
    "    \n",
    "    for m in matches:\n",
    "        a = m.document.get_data().decode('utf-8')\n",
    "        tile_pageid = a.split(' ', 2)[:2]\n",
    "        text = a.split(' ', 2)[2] \n",
    "        if str(tile_pageid[1]).isdigit():\n",
    "            doc.append(dict(\n",
    "                index = index,\n",
    "                claim = claim,\n",
    "                label = label,\n",
    "                evidence = evidence,\n",
    "                ir_lookup = a,\n",
    "                evidence_topk = tile_pageid,\n",
    "                text_topk =text,\n",
    "                percentage = m.percent,\n",
    "                weight = m.weight,            \n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f064b6809b74177847ea6ac7c5d6ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=14997, style=ProgressStyle(description_widâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nun = train_set.swifter.apply(get_topk_evidence,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame(doc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lenGold(df):\n",
    "    check = tuple([df['evidence'], df['evidence_topk'])\n",
    "    if check in set(tuple(i) for i in df['evidence']):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['isGold'] = train_set.apply(get_lenGold, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_sim = []\n",
    "lcs_data = []\n",
    "spacy_sim = []\n",
    "cosine_sim = []\n",
    "ngram = []\n",
    "\n",
    "    \n",
    "for i, val in train_set.iterrows():\n",
    "    claim = val['claim']\n",
    "    evidence = val['text_topk']\n",
    "\n",
    "    #Jaccard Similarity \n",
    "    jacc = jaccardSim(claim, evidence)\n",
    "    jaccard_sim.append(jacc)\n",
    "    \n",
    "    #LCS\n",
    "    lcs_count = lcs(str(claim), str(evidence))\n",
    "    lcs_data.append(lcs_count)\n",
    "    \n",
    "    #spacy simlarity\n",
    "    sim = (nlp(claim)).similarity(nlp(evidence))\n",
    "    spacy_sim.append(sim)\n",
    "    \n",
    "    #cosine_sim_tfidf\n",
    "    document = [claim,evidence]\n",
    "    cosine_sim.append(cosineSimTfidf(document))\n",
    "    \n",
    "    #NGram compare\n",
    "    ngram_comp = NGram.compare(claim,evidence)\n",
    "    ngram.append(ngram_comp)\n",
    "    \n",
    "    \n",
    "train_set['jaccard_sim'] = jaccard_sim\n",
    "train_set['cosine_sim_tfidf'] = cosine_sim\n",
    "train_set['spaCy_similarity'] = spacy_sim\n",
    "train_set['lcs'] = lcs_data\n",
    "train_set['ngram'] = ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('train_set_with_features.csv') #to csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
